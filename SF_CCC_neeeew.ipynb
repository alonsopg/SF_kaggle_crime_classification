{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -d -u -p pyprind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Crime Classification Challenge\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "Currently, supervised classification is becoming a very important problem with a wide range of applications, from sentiment analysis and cancer detection to fraud detection systems, from [authorship profiling systems](https://github.com/alonsopg/AuthorProfiling) to image recognition and loan prediction systems.\n",
    "\n",
    "This [is my solution](http://alonsopg.com/) for the [Kaggle's San Francisco Crime Classification Challenge](https://www.kaggle.com/c/sf-crime). My approach for this task is based on [XGboost's Gradient Boosting implementation](https://arxiv.org/pdf/1603.02754v3.pdf).\n",
    "\n",
    "\n",
    "## The task:\n",
    "\n",
    "This competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given the date, location, district, address, description, and coordinates of historical records the task is about predicting the category of crime that occurred.\n",
    "\n",
    "<img src=\"files/SF_pic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset:\n",
    "\n",
    "The dataset consist in two spreadsheet files with 1762309 instances (rows) of crime reports, which are divided into training and testing chunks. The reason of this \"chunking\" is that we will use the training data to train an algorithm that yields a model that learns to identify new unlabeled, unclassified or unannotated categories of crimes given its other attributes (`Dates, DayOfWeek, PdDistrict, Address, X, and Y coordinates`)\n",
    "\n",
    "### Training data:\n",
    "\n",
    "With regards to the training data, the historical records consist a `.csv` file very similar to a database table. Interestingly, this historical records have a `Category` column that represents the `label` or target mark that represents the category of the crime (`WARRANTS, OTHER OFFENSES, LARCENY/THEFT, VEHICLE THEFT, VANDALISM, NON-CRIMINAL, ROBBERY, ASSAULT, WEAPON LAWS, BURGLARY, SUSPICIOUS OCC, DRUNKENNESS, FORGERY/COUNTERFEITING, DRUG/NARCOTIC, STOLEN PROPERTY, SECONDARY CODES, TRESPASS, MISSING PERSON, FRAUD, KIDNAPPING, RUNAWAY, DRIVING UNDER THE INFLUENCE, SEX OFFENSES FORCIBLE, PROSTITUTION, DISORDERLY CONDUCT, ARSON, FAMILY OFFENSES, LIQUOR LAWS, BRIBERY, EMBEZZLEMENT, SUICIDE, LOITERING, SEX OFFENSES NON FORCIBLE, EXTORTION, GAMBLING, BAD CHECKS, TREA, RECOVERED VEHICLE, PORNOGRAPHY/OBSCENE MAT`)\n",
    "\n",
    "\n",
    "<img src=\"files/pic2.png\">\n",
    "\n",
    "\n",
    "### Testing data:\n",
    "\n",
    "On the other hand, the testing data doesn't have a category column (target).\n",
    "<img src=\"files/pic1.png\">\n",
    "\n",
    "## Gradient Boosting Classification Trees:\n",
    "\n",
    "The state of the art in supervised classification is gr\n",
    "\n",
    "\n",
    "As usual data is represented in form of instances with the $\\{\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "For my solution I'll use a Python programming environment (Numpy, pandas, sklearn, and XGboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>878047</th>\n",
       "      <td>2003-01-06 00:01:00</td>\n",
       "      <td>VANDALISM</td>\n",
       "      <td>MALICIOUS MISCHIEF, VANDALISM OF VEHICLES</td>\n",
       "      <td>Monday</td>\n",
       "      <td>SOUTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>TOWNSEND ST / 2ND ST</td>\n",
       "      <td>-122.390531</td>\n",
       "      <td>37.780607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878048</th>\n",
       "      <td>2003-01-06 00:01:00</td>\n",
       "      <td>FORGERY/COUNTERFEITING</td>\n",
       "      <td>CHECKS, FORGERY (FELONY)</td>\n",
       "      <td>Monday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1800 Block of NEWCOMB AV</td>\n",
       "      <td>-122.394926</td>\n",
       "      <td>37.738212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Dates                Category  \\\n",
       "878047  2003-01-06 00:01:00               VANDALISM   \n",
       "878048  2003-01-06 00:01:00  FORGERY/COUNTERFEITING   \n",
       "\n",
       "                                         Descript DayOfWeek PdDistrict  \\\n",
       "878047  MALICIOUS MISCHIEF, VANDALISM OF VEHICLES    Monday   SOUTHERN   \n",
       "878048                   CHECKS, FORGERY (FELONY)    Monday    BAYVIEW   \n",
       "\n",
       "       Resolution                   Address           X          Y  \n",
       "878047       NONE      TOWNSEND ST / 2ND ST -122.390531  37.780607  \n",
       "878048       NONE  1800 Block of NEWCOMB AV -122.394926  37.738212  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Firstly let's read the training data:\n",
    "import pandas as pd\n",
    "training_data  = pd.read_csv('/Users/user/Jupyter/datasets/SFCCC_dataset/train 2.csv')\n",
    "training_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the `Dates` column into two columns (`Dates` and `Time`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>878047</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>VANDALISM</td>\n",
       "      <td>MALICIOUS MISCHIEF, VANDALISM OF VEHICLES</td>\n",
       "      <td>Monday</td>\n",
       "      <td>SOUTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>TOWNSEND ST / 2ND ST</td>\n",
       "      <td>-122.390531</td>\n",
       "      <td>37.780607</td>\n",
       "      <td>00:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878048</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>FORGERY/COUNTERFEITING</td>\n",
       "      <td>CHECKS, FORGERY (FELONY)</td>\n",
       "      <td>Monday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1800 Block of NEWCOMB AV</td>\n",
       "      <td>-122.394926</td>\n",
       "      <td>37.738212</td>\n",
       "      <td>00:01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dates                Category  \\\n",
       "878047  2003-01-06               VANDALISM   \n",
       "878048  2003-01-06  FORGERY/COUNTERFEITING   \n",
       "\n",
       "                                         Descript DayOfWeek PdDistrict  \\\n",
       "878047  MALICIOUS MISCHIEF, VANDALISM OF VEHICLES    Monday   SOUTHERN   \n",
       "878048                   CHECKS, FORGERY (FELONY)    Monday    BAYVIEW   \n",
       "\n",
       "       Resolution                   Address           X          Y      Time  \n",
       "878047       NONE      TOWNSEND ST / 2ND ST -122.390531  37.780607  00:01:00  \n",
       "878048       NONE  1800 Block of NEWCOMB AV -122.394926  37.738212  00:01:00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"Dates\"], training_data[\"Time\"] = zip(*training_data[\"Dates\"].str.split().tolist())\n",
    "training_data.tail(2)\n",
    "# If we wanted to delete the dates or time column:\n",
    "#del training_data[\"Dates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>15:58:00</td>\n",
       "      <td>STOLEN AND RECOVERED VEHICLE</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>2400 Block of CALIFORNIA ST</td>\n",
       "      <td>-122.434689</td>\n",
       "      <td>37.788854</td>\n",
       "      <td>VEHICLE THEFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>DRIVERS LICENSE, SUSPENDED OR REVOKED</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>MISSION</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>23RD ST / VERMONT ST</td>\n",
       "      <td>-122.403525</td>\n",
       "      <td>37.754453</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dates      Time                               Descript  DayOfWeek  \\\n",
       "48  2015-05-13  15:58:00           STOLEN AND RECOVERED VEHICLE  Wednesday   \n",
       "49  2015-05-13  17:28:00  DRIVERS LICENSE, SUSPENDED OR REVOKED  Wednesday   \n",
       "\n",
       "   PdDistrict      Resolution                      Address           X  \\\n",
       "48   NORTHERN            NONE  2400 Block of CALIFORNIA ST -122.434689   \n",
       "49    MISSION  ARREST, BOOKED         23RD ST / VERMONT ST -122.403525   \n",
       "\n",
       "            Y        Category  \n",
       "48  37.788854   VEHICLE THEFT  \n",
       "49  37.754453  OTHER OFFENSES  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's read the test data:\n",
    "testing_data = pd.read_csv('/Users/user/Desktop/neew.csv')\n",
    "#Delete the index column\n",
    "testing_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the previous procedure just to give coherence to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Por el momento me lo salto, esto debido a que lo verificaremos a mano\n",
    "testing_data[\"Dates\"], testing_data[\"Time\"] = zip(*testing_data[\"Dates\"].str.split().tolist())\n",
    "testing_data.tail(2)\n",
    "# If we wanted to delete the dates or time column:\n",
    "#del training_data[\"Dates\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "\n",
    "#Categorical values\n",
    "\n",
    "#district_le = LabelEncoder().fit_transform(training_data['PdDistrict'].values.ravel().reshape(-1, 1))\n",
    "#dates_le = LabelEncoder().fit_transform(training_data['Dates'].values.ravel().reshape(-1, 1))\n",
    "#day_of_week_le = LabelEncoder().fit_transform(training_data['DayOfWeek'].values.ravel().reshape(-1, 1))\n",
    "\n",
    "#district_le = LabelEncoder().fit_transform(training_data['PdDistrict'].reshape(-1, 1))\n",
    "#dates_le = LabelEncoder().fit_transform(training_data['Dates'].values.reshape(-1, 1))\n",
    "#day_of_week_le = LabelEncoder().fit_transform(training_data['DayOfWeek'].reshape(-1, 1))\n",
    "\n",
    "district_le = LabelEncoder().fit_transform(training_data['PdDistrict'].values.reshape(-1, 1).ravel())\n",
    "dates_le = LabelEncoder().fit_transform(training_data['Dates'].values.reshape(-1, 1).ravel())\n",
    "day_of_week_le = LabelEncoder().fit_transform(training_data['DayOfWeek'].values.reshape(-1, 1).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878049,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_of_week_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "district_feats = OneHotEncoder().fit_transform(district_le.reshape(-1, 1)).A\n",
    "dates_feats = OneHotEncoder().fit_transform(dates_le.reshape(-1, 1)).A\n",
    "day_of_week_feats = OneHotEncoder().fit_transform(day_of_week_le.reshape(-1, 1)).A\n",
    "#words\n",
    "descript_feats = CountVectorizer().fit_transform(training_data['Descript'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 10) (878049, 2249) (878049, 7)\n"
     ]
    }
   ],
   "source": [
    "print(district_feats.shape, dates_feats.shape, day_of_week_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#print(district_feats,'\\n', dates_feats,'\\n', day_of_week_feats)\n",
    "#print(10*'\\n*\\n')\n",
    "print(district_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sparse\n",
    "import numpy as np\n",
    "\n",
    "#X_combined_features = np.vstack((descript_feats.A.T, district_feats, dates_feats, day_of_week_feats)).T\n",
    "\n",
    "#X_combined_features = np.vstack((descript_feats.A.T, \n",
    "#                                 district_feats, \n",
    "#                                 dates_feats, day_of_week_feats)).T\n",
    "\n",
    "\n",
    "#descript_feats is sparse turn it into a array with .A\n",
    "X_combined_features =  np.column_stack((descript_feats.A,\n",
    "                                        district_feats, dates_feats, \n",
    "                                        day_of_week_feats))\n",
    "\n",
    "\n",
    "y = LabelEncoder().fit_transform(training_data['Category'].values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.partial_fit(X_combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_combined_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_combined_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = False)\n",
    "\n",
    "clf_pipeline = make_pipeline(StandardScaler(), XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "        #Features\n",
    "        ('features', FeatureUnion(transformer_list=[\n",
    "                    ('descript', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,0], validate=False)),\n",
    "                                ('tfidf', CountVectorizer(binary=True)),\n",
    "                            ])),\n",
    "                    ('district', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[1]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: district_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('resolution', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[2]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: resolution_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('dates', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[3]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: dates_le.transform(X.ravel()).reshape(-1, 1), validate=False)), \n",
    "                            ])),\n",
    "                    \n",
    "                    ('dayOfWeek', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[4]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: day_of_week_le.transform(X.ravel()).reshape(-1, 1), validate=False)), \n",
    "                            ])), \n",
    "                ])),\n",
    "        #Estimator\n",
    "        ('est', XGBClassifier(n_estimators=30, max_depth=2))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = training_data[['Descript', 'PdDistrict', 'Resolution', 'Dates', 'DayOfWeek']].values\n",
    "\n",
    "\n",
    "categories_le = LabelEncoder()\n",
    "y = categories_le.fit_transform(training_data['Category'].values.ravel())\n",
    "\n",
    "\n",
    "kfold = KFold(n=len(y), n_folds=10, random_state=False)\n",
    "\n",
    "results = cross_val_score(clf, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: %.2f%% \" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "        #Features\n",
    "        ('features', FeatureUnion(transformer_list=[\n",
    "                    ('descript', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,0], validate=False)),\n",
    "                                ('tfidf', CountVectorizer(binary=True)),\n",
    "                            ])),\n",
    "                    ('district', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[1]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: district_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('resolution', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[2]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: resolution_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('dates', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[3]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: dates_le.transform(X.ravel()).reshape(-1, 1), validate=False)), \n",
    "                            ])),\n",
    "                    \n",
    "                    ('dayOfWeek', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[4]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: day_of_week_le.transform(X.ravel()).reshape(-1, 1), validate=False)), \n",
    "                            ])), \n",
    "                ])),\n",
    "        #Estimator\n",
    "        ('est', XGBClassifier(n_estimators=30, max_depth=2))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = training_data[['Descript', 'PdDistrict', 'Resolution', 'Dates', 'DayOfWeek']].values\n",
    "\n",
    "\n",
    "categories_le = LabelEncoder()\n",
    "y = categories_le.fit_transform(training_data['Category'].values.ravel())\n",
    "\n",
    "\n",
    "kfold = KFold(n=len(y), n_folds=10, random_state=False)\n",
    "\n",
    "results = cross_val_score(clf, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After the loading train data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "district_le = LabelEncoder().fit(training_data['PdDistrict'].values.ravel())\n",
    "resolution_le = LabelEncoder().fit(training_data['Resolution'].values.ravel())\n",
    "clf = Pipeline([\n",
    "        ('features', FeatureUnion(transformer_list=[\n",
    "                    ('descript', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,0], validate=False)),\n",
    "                                ('tfidf', CountVectorizer(binary=True)),\n",
    "                            ])),\n",
    "                    ('district', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[1]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: district_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('resolution', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[2]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: resolution_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                ])),\n",
    "        ('est', XGBClassifier(n_estimators=30, max_depth=2)),\n",
    "    ])\n",
    "categories_le = LabelEncoder()\n",
    "X = training_data[['Descript', 'PdDistrict', 'Resolution']].values\n",
    "y = categories_le.fit_transform(training_data['Category'].values.ravel())\n",
    "print(cross_val_score(clf, X, y, cv=3, verbose=3))\n",
    "\n",
    "\n",
    "# Operate\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "district_le = LabelEncoder().fit(training_data['PdDistrict'].values.ravel())\n",
    "resolution_le = LabelEncoder().fit(training_data['Resolution'].values.ravel())\n",
    "categories_le = LabelEncoder()\n",
    "X = training_data[['Descript', 'PdDistrict', 'Resolution']].values\n",
    "y = categories_le.fit_transform(training_data['Category'].values.ravel())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "clf = Pipeline([\n",
    "        ('features', FeatureUnion(transformer_list=[\n",
    "                    ('descript', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,0], validate=False)),\n",
    "                                ('tfidf', CountVectorizer(binary=True)),\n",
    "                            ])),\n",
    "                    ('district', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[1]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: district_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                    ('resolution', Pipeline([\n",
    "                                ('select', FunctionTransformer(lambda X: X[:,[2]], validate=False)),\n",
    "                                ('transform', FunctionTransformer(lambda X: resolution_le.transform(X.ravel()).reshape(-1,1), validate=False)),\n",
    "                            ])),\n",
    "                ])),\n",
    "        ('est', XGBClassifier(n_estimators=30, max_depth=2)),\n",
    "    ])\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.predict(X_test))\n",
    "print(categories_le.inverse_transform(clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
